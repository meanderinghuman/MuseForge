# ğŸ’» MuseForge  
> *Where Artificial Intelligence forges stories, heritage, and human creativity into digital form.*

---

<div align="center">

![GitHub stars](https://img.shields.io/github/stars/meanderinghuman/MuseForge?style=social)
![GitHub forks](https://img.shields.io/github/forks/meanderinghuman/MuseForge?style=social)
![GitHub issues](https://img.shields.io/github/issues/meanderinghuman/MuseForge)
![GitHub pull requests](https://img.shields.io/github/issues-pr/meanderinghuman/MuseForge)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![Framework](https://img.shields.io/badge/framework-PyTorch-red)
![Model](https://img.shields.io/badge/model-Qwen2--VL--2B-orange)

**An intelligent AI system that â€œseesâ€ cultural artifacts and forges authentic stories reflecting the essence of human heritage.**

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“Š Highlights](#-project-highlights) â€¢ [ğŸ—ï¸ Architecture](#-technical-architecture) â€¢ [ğŸ§  Dataset](#-dataset-format) â€¢ [ğŸ¤ Contributing](#-contributing) â€¢ [ğŸ“œ License](#-license)

</div>

---

## ğŸŒŸ Overview  

**MuseForge** is an advanced AI framework that combines *machine perception* and *creative storytelling* to bring cultural and visual data to life.  
By uniting **Vision-Language Models (VLMs)** and **progressive learning**, it transforms silent imagery â€” temples, monuments, attire, and art â€” into rich, context-aware narratives that preserve cultural depth and authenticity.

It acts as a *digital forge*, shaping **pixels into prose** and **data into meaning.**

---

## ğŸ§  Core Philosophy  

- **ğŸ” Perception:** Empowering AI to see and recognize cultural depth in imagery.  
- **ğŸ§© Interpretation:** Understanding regional context and artistic nuance.  
- **ğŸª¶ Creation:** Generating authentic, human-like cultural stories with AI.

---

## ğŸ“Š Project Highlights  

<div align="center">

| Feature | Description |  
|----------|--------------|  
| ğŸ§  **AI Core** | Vision-Language Model (Qwen2-VL, 2B parameters) fine-tuned via LoRA for cultural storytelling. |  
| ğŸ§± **3-Stage Training Pipeline** | Recognition â†’ Comprehension â†’ Story Generation â€” progressive fine-tuning for semantic depth. |  
| ğŸ¨ **Cultural Intelligence** | Detects regional and artistic attributes to create authentic heritage narratives. |  
| âš™ï¸ **Model Efficiency** | 85% reduction in loss (130 â†’ 19.7) with LoRA + quantization. |  
| ğŸ” **Explainability** | Traceable mapping between visual features and generated text. |  
| ğŸŒ **Impact** | Aids digital preservation of heritage through AI-driven storytelling. |  

</div>

---

## ğŸ—ï¸ Technical Architecture  

```mermaid
graph TD
    A[ğŸ–¼ï¸ Image Input] --> B[ğŸ‘ï¸ Qwen2-VL Vision Encoder]
    B --> C[ğŸ§  Language Model Understanding]
    C --> D[ğŸ§© Stage 1: Cultural Recognition]
    D --> E[ğŸ§­ Stage 2: Pattern & Context Understanding]
    E --> F[ğŸª¶ Stage 3: Story Generation]
    F --> G[ğŸ“– Authentic Cultural Narrative]
```

---

## âš¡ Training Configuration  

| Parameter | Value |  
|------------|--------|  
| **Base Model** | Qwen2-VL-2B-Instruct |  
| **Fine-tuning** | LoRA (r=8, Î±=32) |  
| **Precision** | 4-bit Quantization (bfloat16) |  
| **Optimizer** | AdamW + Cosine Scheduler |  
| **Context Length** | 512 tokens |  
| **Training Time** | ~3â€“5 hours (8GB GPU) |  

---

## ğŸ“ Repository Structure  

```bash
MuseForge/
â”œâ”€â”€ ğŸ§  train_pipeline.py        # Main 3-stage training pipeline
â”œâ”€â”€ ğŸ”® inference.py             # Story generation module
â”œâ”€â”€ ğŸ§© dataset_utils.py         # Dataset processing & augmentation
â”œâ”€â”€ âš™ï¸ config.py                # Configurations and hyperparameters
â”œâ”€â”€ ğŸ“Š evaluation_metrics.py    # Narrative accuracy evaluation
â”œâ”€â”€ ğŸ“‹ requirements.txt         # Dependencies
â”œâ”€â”€ ğŸ“ data/                    # Dataset directory
â”‚   â””â”€â”€ cultural_dataset.json   # JSON-formatted dataset
â”œâ”€â”€ ğŸ“ models/                  # Model checkpoints
â”‚   â””â”€â”€ museforge_vlm_model/    # Trained model folder
â”œâ”€â”€ ğŸ“ examples/                # Example inputs & outputs
â”‚   â”œâ”€â”€ input_image.jpg
â”‚   â””â”€â”€ generated_story.txt
â””â”€â”€ ğŸ“– README.md
```

---

## ğŸš€ Quick Start  

### 1ï¸âƒ£ Installation  

```bash
git clone https://github.com/meanderinghuman/MuseForge.git
cd MuseForge
pip install -r requirements.txt
pip install torch torchvision transformers[vision] peft accelerate bitsandbytes
```

---

### 2ï¸âƒ£ Usage  

```python
from inference import MuseForgeModel

model = MuseForgeModel("./models/museforge_vlm_model")
story = model.generate_story("examples/input_image.jpg")
print(story)
```

**Example Output:**
```
This timeless South Indian temple, adorned with intricate carvings and a towering gopuram, 
echoes centuries of devotion and craftsmanship â€” a living narrative of culture forged in stone.
```

---

### 3ï¸âƒ£ Train Your Own Model  

```bash
# Run full progressive training
python train_pipeline.py

# Test with smaller samples
python train_pipeline.py --test_samples 100

# Single stage training (optional)
python train_pipeline.py --single_stage
```

---

## ğŸ§  Dataset Format  

MuseForge expects datasets in this JSON structure:  

```json
{
  "image_path": "regional_images/South_India/temple_001.jpg",
  "region": "South_India",
  "metadata": {
    "elements": ["temple", "gopuram", "sculpture"],
    "architecture": {"style": "Dravidian"}
  },
  "context_story": "This ancient temple stands as a testament to South Indian art and devotion."
}
```

---

## ğŸŒ Regional Dataset Distribution  

| Region | Samples | Focus |  
|---------|----------|----------------|  
| ğŸ”ï¸ **North India** | 160 | Temples, monuments |  
| ğŸ›ï¸ **South India** | 200 | Dravidian architecture |  
| ğŸ­ **East India** | 200 | Colonial heritage |  
| ğŸœï¸ **West India** | 200 | Stepwells, forts |  
| ğŸŒ„ **Northeast India** | 200 | Tribal and ecological art |  

---

## ğŸ”¬ Research Impact  

MuseForge contributes to **AI-driven cultural preservation**, bridging technology with tradition.  
It can serve as a **research framework** for:
- Vision-language grounding tasks  
- Cross-cultural narrative synthesis  
- Heritage digitization and multimodal learning  

---

## ğŸ¤ Contributing  

We welcome contributions from AI researchers, developers, and cultural experts!  

1. ğŸ´ Fork the repository  
2. ğŸŒ¿ Create a branch: `git checkout -b feature/your-feature`  
3. ğŸ’¾ Commit: `git commit -m 'Added feature'`  
4. ğŸ“¤ Push: `git push origin feature/your-feature`  
5. ğŸ”€ Submit a Pull Request  

**Areas to contribute:**  
- Dataset expansion (regional or artifact-level)  
- Model architecture improvements  
- Multilingual or cross-cultural story support  
- UI or demo app integrations  

---

## ğŸ“œ License  

This project is licensed under the **MIT License** â€” you are free to use, modify, and distribute the software with attribution.  

```
MIT License

Copyright (c) 2025 Siddharth Pal

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the â€œSoftwareâ€), to deal
in the Software without restriction...
```

---

## ğŸ™ Acknowledgments  

- **Qwen Team** â€” for the Qwen2-VL Vision-Language model  
- **Open Source AI Community** â€” for enabling multimodal research and tools  
- **Cultural Heritage Scholars** â€” for inspiring the integration of tradition and technology  

---

<div align="center">

â­ *If MuseForge inspired you â€” consider starring this repository to support open cultural AI research.*  

</div>
